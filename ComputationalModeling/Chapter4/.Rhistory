source("GCMprednoisy.R")
library(dfoptim)
# A function to get deviance from GCM
GCMutil <- function(theta, stim, exemplars, data, N, retpreds){
nDat <- length(data)
dev <- rep(NA, nDat)
preds <- dev
c <- theta[1]
w <- theta[2]
w[2] <- (1-w[1])*theta[3]
w[3] <- (1-sum(w[1:2]))*theta[4]
w[4] <- (1-sum(w[1:3]))
sigma <- theta[5]
b <- theta[6]
for (i in 1:nDat){
p <- GCMprednoisy(stim[i,], exemplars, c, w, sigma, b)
dev[i] <- -2*log(dbinom(data[i] ,size = N,prob = p[1]))
preds[i] <- p[1]
}
if (retpreds){
return(preds)
} else {
return(sum(dev))
}
}
N <- 2*40 # there were 2 responses per face from 40 ppl
stim <- as.matrix(read.table("faceStim.csv", sep=",")) #* \label{line:MaximumLikelihood:readFaces_noisy}  *\#
exemplars <- list(a=stim[1:5,], b= stim[6:10,]) #* \label{line:MaximumLikelihood:assignExemplars_noisy}  *\#
data <- scan(file="facesDataLearners.txt")
data <- ceiling(data*N)
bestfit <- 10000
for (w1 in c(0.25,0.5,0.75)){ #* \label{line:GCMLoopBegin}  *\#
for (w2 in c(0.25,0.5,0.75)){
for (w3 in c(0.25,0.5,0.75)){
print(c(w1,w2,w3))
fitres <- nmkb(par=c(1,w1,w2,w3,1,0.2),
fn = function(theta) GCMutil(theta,stim,exemplars,data, N, FALSE),
lower=c(0,0,0,0,0,-5),
upper=c(10,1,1,1,10,5),
control=list(trace=0))
print(fitres)
if (fitres$value<bestfit){
bestres <- fitres
bestfit <- fitres$value
}
}
}
} #* \label{line:GCMLoopEnd}  *\#
preds <- GCMutil(bestres$par,stim,exemplars,data, N, TRUE)
#pdf(file="GCMfits.pdf", width=5, height=5)
plot(preds,data/N,
xlab="Data", ylab="Predictions")
#dev.off()
print(bestres)
theta <- bestres$par
w <- theta[2]
w[2] <- (1-w[1])*theta[3]
w[3] <- (1-sum(w[1:2]))*theta[4]
w[4] <- (1-sum(w[1:3]))
print(w)
print(w)
source("GCMpred.R")
N <- 2*80 # there were 2 responses per face from 80 ppl
N_A <- round(N*.968) #N_B is implicitly N - N_A
c <- 4
w <- c(0.19, 0.12, 0.25, 0.45)
stim <- as.matrix(read.table("faceStim.csv", sep=",")) #* \label{line:MaximumLikelihood:readFaces}  *\#
exemplars <- list(a=stim[1:5,], b= stim[6:10,]) #* \label{line:MaximumLikelihood:assignExemplars}  *\#
preds <- GCMpred(stim[1,], exemplars, c, w)
likelihood <- dbinom(N_A ,size = N,prob = preds[1])
#preds <- t(apply(testProbe, 1, function(x) GCMpred(x, exemplars, c, w)))
#likelihood <- GCMlik(N_A, N, stim[1,], exemplars, c, w)
likelihood
preds <- t(apply(testProbe, 1, function(x) GCMpred(x, exemplars, c, w)))
likelihood
GCMpred <- function(probe, exemplars, c, w){
# calculate likelihod of N_A `A' responses out of N given parameter c
# 'stim' is a single vector representing the stimulus to be categorised
# 'exemplars' is a list of exemplars; the first list item is the 'A' exemplars
#   in memory, and the second list item is the `B` exemplars in memory
#   each list item is a matrix in which the rows correspond to individual
#   exemplars
# 'c' is the scaling parameter, and 'w' is a vector giving weighting for each
#   stimulus dimension (the columns in 'stim' and 'exemplars')
# note: for a large number of categories we could use lapply to loop across
# the categories in the list 'exemplars'
dist <- list() #* \label{line:MaximumLikelihood:initdist}  *\#
for (ex in exemplars){ #* \label{line:MaximumLikelihood:exemplarloop}  *\#
dist[[length(dist)+1]] <- apply(as.array(ex), 1,
function(x) sqrt(sum(w*(x-probe)^2)))
}
sumsim <- lapply(dist, function(a) sum(exp(-c*a)))
r_prob <- unlist(sumsim)/sum(unlist(sumsim))
}
GCMprednoisy <- function(probe, exemplars, c, w, sigma, b){
# calculate likelihod of N_A `A' responses out of N given parameter c
# 'stim' is a single vector representing the stimulus to be categorised
# 'exemplars' is a list of exemplars; the first list item is the 'A' exemplars
#   in memory, and the second list item is the `B` exemplars in memory
#   each list item is a matrix in which the rows correspond to individual
#   exemplars
# 'c' is the scaling parameter, and 'w' is a vector giving weighting for each
#   stimulus dimension (the columns in 'stim' and 'exemplars')
# note: for a large number of categories we could use lapply to loop across
# the categories in the list 'exemplars'
dist <- list() #* \label{line:MaximumLikelihood:initdist_n}  *\#
for (ex in exemplars){ #* \label{line:MaximumLikelihood:exemplarloop_n}  *\#
dist[[length(dist)+1]] <- apply(as.array(ex), 1,
function(x) sqrt(sum(w*(x-probe)^2)))
}
sumsim <- unlist(lapply(dist, function(a) sum(exp(-c*a))))
# this only works for 2 categories
# we also simplify Nosofsky model in only applying noise at the end
r_prob <- c(0,0)
r_prob[1] <- pnorm(sumsim[1]-sumsim[2]-b,sd=sigma)
r_prob[2] <- 1 - r_prob[1]
return(r_prob)
}
rswald <- function(t, a, m, Ter){
ans <- a/sqrt(2*pi*(t-Ter)^3)*
exp(-(a-m*(t-Ter))^2/(2*(t-Ter)))
}
source("GCMprednoisy.R")
library(dfoptim)
# A function to get deviance from GCM
GCMutil <- function(theta, stim, exemplars, data, N, retpreds){
nDat <- length(data)
dev <- rep(NA, nDat)
preds <- dev
c <- theta[1]
w <- theta[2]
w[2] <- (1-w[1])*theta[3]
w[3] <- (1-sum(w[1:2]))*theta[4]
w[4] <- (1-sum(w[1:3]))
sigma <- theta[5]
b <- theta[6]
for (i in 1:nDat){
p <- GCMprednoisy(stim[i,], exemplars, c, w, sigma, b)
dev[i] <- -2*log(dbinom(data[i] ,size = N,prob = p[1]))
preds[i] <- p[1]
}
if (retpreds){
return(preds)
} else {
return(sum(dev))
}
}
N <- 2*40 # there were 2 responses per face from 40 ppl
stim <- as.matrix(read.table("faceStim.csv", sep=",")) #* \label{line:MaximumLikelihood:readFaces_noisy}  *\#
exemplars <- list(a=stim[1:5,], b= stim[6:10,]) #* \label{line:MaximumLikelihood:assignExemplars_noisy}  *\#
data <- scan(file="facesDataLearners.txt")
data <- ceiling(data*N)
bestfit <- 10000
for (w1 in c(0.25,0.5,0.75)){ #* \label{line:GCMLoopBegin}  *\#
for (w2 in c(0.25,0.5,0.75)){
for (w3 in c(0.25,0.5,0.75)){
print(c(w1,w2,w3))
fitres <- nmkb(par=c(1,w1,w2,w3,1,0.2),
fn = function(theta) GCMutil(theta,stim,exemplars,data, N, FALSE),
lower=c(0,0,0,0,0,-5),
upper=c(10,1,1,1,10,5),
control=list(trace=0))
print(fitres)
if (fitres$value<bestfit){
bestres <- fitres
bestfit <- fitres$value
}
}
}
} #* \label{line:GCMLoopEnd}  *\#
preds <- GCMutil(bestres$par,stim,exemplars,data, N, TRUE)
#pdf(file="GCMfits.pdf", width=5, height=5)
plot(preds,data/N,
xlab="Data", ylab="Predictions")
#dev.off()
print(bestres)
theta <- bestres$par
w <- theta[2]
w[2] <- (1-w[1])*theta[3]
w[3] <- (1-sum(w[1:2]))*theta[4]
w[4] <- (1-sum(w[1:3]))
print(w)
source("GCMpred.R")
N <- 2*80 # there were 2 responses per face from 80 ppl
N_A <- round(N*.968) #N_B is implicitly N - N_A
c <- 4
w <- c(0.19, 0.12, 0.25, 0.45)
stim <- as.matrix(read.table("faceStim.csv", sep=",")) #* \label{line:MaximumLikelihood:readFaces}  *\#
exemplars <- list(a=stim[1:5,], b= stim[6:10,]) #* \label{line:MaximumLikelihood:assignExemplars}  *\#
preds <- GCMpred(stim[1,], exemplars, c, w)
likelihood <- dbinom(N_A ,size = N,prob = preds[1])
#preds <- t(apply(testProbe, 1, function(x) GCMpred(x, exemplars, c, w)))
#likelihood <- GCMlik(N_A, N, stim[1,], exemplars, c, w)
likelihood
preds
GCMpred <- function(probe, exemplars, c, w){
# calculate likelihod of N_A `A' responses out of N given parameter c
# 'stim' is a single vector representing the stimulus to be categorised
# 'exemplars' is a list of exemplars; the first list item is the 'A' exemplars
#   in memory, and the second list item is the `B` exemplars in memory
#   each list item is a matrix in which the rows correspond to individual
#   exemplars
# 'c' is the scaling parameter, and 'w' is a vector giving weighting for each
#   stimulus dimension (the columns in 'stim' and 'exemplars')
# note: for a large number of categories we could use lapply to loop across
# the categories in the list 'exemplars'
dist <- list() #* \label{line:MaximumLikelihood:initdist}  *\#
for (ex in exemplars){ #* \label{line:MaximumLikelihood:exemplarloop}  *\#
dist[[length(dist)+1]] <- apply(as.array(ex), 1,
function(x) sqrt(sum(w*(x-probe)^2)))
}
sumsim <- lapply(dist, function(a) sum(exp(-c*a)))
r_prob <- unlist(sumsim)/sum(unlist(sumsim))
}
GCMprednoisy <- function(probe, exemplars, c, w, sigma, b){
# calculate likelihod of N_A `A' responses out of N given parameter c
# 'stim' is a single vector representing the stimulus to be categorised
# 'exemplars' is a list of exemplars; the first list item is the 'A' exemplars
#   in memory, and the second list item is the `B` exemplars in memory
#   each list item is a matrix in which the rows correspond to individual
#   exemplars
# 'c' is the scaling parameter, and 'w' is a vector giving weighting for each
#   stimulus dimension (the columns in 'stim' and 'exemplars')
# note: for a large number of categories we could use lapply to loop across
# the categories in the list 'exemplars'
dist <- list() #* \label{line:MaximumLikelihood:initdist_n}  *\#
for (ex in exemplars){ #* \label{line:MaximumLikelihood:exemplarloop_n}  *\#
dist[[length(dist)+1]] <- apply(as.array(ex), 1,
function(x) sqrt(sum(w*(x-probe)^2)))
}
sumsim <- unlist(lapply(dist, function(a) sum(exp(-c*a))))
# this only works for 2 categories
# we also simplify Nosofsky model in only applying noise at the end
r_prob <- c(0,0)
r_prob[1] <- pnorm(sumsim[1]-sumsim[2]-b,sd=sigma)
r_prob[2] <- 1 - r_prob[1]
return(r_prob)
}
rswald <- function(t, a, m, Ter){
ans <- a/sqrt(2*pi*(t-Ter)^3)*
exp(-(a-m*(t-Ter))^2/(2*(t-Ter)))
}
source("GCMpred.R")
N <- 2*80 # there were 2 responses per face from 80 ppl
N_A <- round(N*.968) #N_B is implicitly N - N_A
c <- 4
w <- c(0.19, 0.12, 0.25, 0.45)
stim <- as.matrix(read.table("faceStim.csv", sep=",")) #* \label{line:MaximumLikelihood:readFaces}  *\#
exemplars <- list(a=stim[1:5,], b= stim[6:10,]) #* \label{line:MaximumLikelihood:assignExemplars}  *\#
preds <- GCMpred(stim[1,], exemplars, c, w)
likelihood <- dbinom(N_A ,size = N,prob = preds[1])
#preds <- t(apply(testProbe, 1, function(x) GCMpred(x, exemplars, c, w)))
#likelihood <- GCMlik(N_A, N, stim[1,], exemplars, c, w)
#dat <- read.csv(file="rt_data.csv") # uncomment this if you read in data
nsubj <- 30
nobs <- 20
q_p <- c(.1,.3,.5,.7,.9)
shift <- rnorm(nsubj,250,50)
scale <- rnorm(nsubj,200,50)
shape <- rnorm(nsubj,2,0.25)
params <- rbind(shift,scale,shape)
print(rowMeans(params))
# rows are participants, columns are observations
dat <- apply(params, 2, function(x) rweibull(nobs,shape=x[3],scale=x[2])+x[1]) #* \label{line:MultipleParticipants:gendata}  *\#
# calculate sample quantiles for each particpant
kk <- apply(dat, 2, function(x) quantile(x, probs=q_p))
## FITTING VIA QUANTILE AVERAGING
# average the quantiles
vinq <- rowMeans(kk)
# fit the shifted Weibull to averaged quantiles
weib_qdev <- function(x,q_emp, q_p){
if (any(x<=0)){
return(10000000)
}
q_pred <- qweibull(q_p,shape=x[3],scale=x[2])+x[1] #* \label{line:MultipleParticipants:qweibull}  *\#
dev <- sqrt(mean((q_pred-q_emp)^2))
}
res <- optim(c(225,225,1),  #* \label{line:MultipleParticipants:qfit}  *\#
function(x) weib_qdev(x, vinq, q_p))
print(res)
## FITTING INDIVIDUAL PARTICIPANTS
weib_deviance <- function(x,rts){ #* \label{line:MultipleParticipants:weib_deviance}  *\#
if (any(x<=0) || any(rts<x[1])){
return(10000000)
}
likel <- dweibull(rts-x[1],shape=x[3],scale=x[2])
dev <- sum(-2*log(likel))
}
res <- apply(dat,2,function(a) optim(c(100,225,1),#* \label{line:MultipleParticipants:run_deviance}  *\#
function(x) weib_deviance(x, a)))
# Extract parameter estimates and put in to a matrix
parest <- matrix(
unlist(lapply(res, function(x) x$par)),
ncol=3, byrow=T)
print(colMeans(parest)) # mean parameter estimates
print(apply(parest,2,sd)) # SD of estimates
# note correlations between parameter estimates
library("ggplot2")
# generate some data
set.seed(1540614451)
N <- 1000
pShort <- 0.3
genpars <- list(c(100,10),
c(150,20))
# we assume equal sampling probability for the three distributions
whichD <- sample(c(1,2),N, replace=TRUE, prob=c(pShort, 1-pShort))
dat <- sapply(whichD, function(x)
rnorm(1,genpars[[x]][1],genpars[[x]][2]))
# function needed in EM
weighted.sd <- function(x,w,mu=mean(x)){
wvar <- sum(w*(x-mu)^2)/
sum(w)
return(sqrt(wvar))
}
# guess parameters
mu1 <- mean(dat,1)*0.8
mu2 <- mean(dat, 1)*1.2
sd1 <- sd(dat)
sd2 <- sd(dat)
ppi <- 0.5
oldppi <- 0
while (abs(ppi-oldppi)>.00001){ #* \label{line:MultipleParticipants:gmmloop}  *\#
oldppi <- ppi
# E step
resp <- ppi*dnorm(dat,mu2,sd2)/
((1-ppi)*dnorm(dat,mu1,sd1) + ppi*dnorm(dat,mu2,sd2))
# M step
mu1 <- weighted.mean(dat,1-resp)
mu2 <- weighted.mean(dat,resp)
sd1 <- weighted.sd(dat,1-resp,mu1)
sd2 <- weighted.sd(dat,resp,mu2)
ppi <- mean(resp)
print(ppi)
}
df <- data.frame(rt=dat)
#pdf(file="GMMexample.pdf", width=5, height=4)
ggplot(df, aes(x = rt)) +
geom_histogram(aes(y = ..density..),colour = "black", fill = "white",
binwidth = 3) +
stat_function(fun = function(k) (1-ppi)*dnorm(k,mu1,sd1)) +
stat_function(fun = function(k) ppi*dnorm(k,mu2,sd2)) +
xlab("RT (ms)") + ylab("Density")
#dev.off()
# mixtools for comparison
library(mixtools) # you'll need to install this library
myEM <- normalmixEM( dat, mu = c(1,4),
sigma=c(sd(dat),sd(dat)))
# Read in the data
# Rows are participants, columns are serial positions
# spcdat <- read.table("freeAccuracy.txt")
# Or generate some example data
nPrim <- 25
nRec <- 50
nBoth <- 25
ll <- 12
serpos <- 1:ll
nTrials <- 10
primDat <- matrix(rep(0,ll*nPrim),nPrim,ll)
for (j in 1:nPrim){
asym <- 0.3
expp <- 1
tdat <- (1-asym)*exp(-expp*(serpos-1)) + asym
primDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}
recDat <- matrix(rep(0,ll*nRec),nRec,ll)
for (j in 1:nRec){
asym <- 0.3
expp <- 1
tdat <- (1-asym)*exp(-expp*rev(serpos-1)) + asym
recDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}
bothDat <- matrix(rep(0,ll*nBoth),nBoth,ll)
for (j in 1:nBoth){
asym <- 0.5
expp <- 1
pc <- 0.5 * exp(-expp*rev(serpos-1)) + 0.5 * exp(-expp*(serpos-1))
tdat <- (1-asym)*pc + asym
bothDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}
spcdat <- rbind(primDat,recDat,bothDat)
#------------------------------------------
#pdf(file="gap_plot.pdf", width=4, height=4)
par(mfrow=c(1,1))
library(cluster) #* \label{line:MultipleParticipants:libclust}  *\#
gskmn <- clusGap(spcdat, FUN = kmeans, nstart = 20, K.max = 8, B=500)
plot(gskmn, ylim=c(0.15, 0.5))
#dev.off()
#-------------------------------------------
#pdf(file="kmeansSPC.pdf", width=8, height=4)
par(mfrow=c(1,2))
plot(colMeans(spcdat), ylim=c(0,1), type="b",
xlab="Serial Position", ylab="Proportion Correct", main=NULL)
kmres <- kmeans(spcdat, centers=3, nstart=10) #* \label{line:MultipleParticipants:runkmeans}  *\#
matplot(t(kmres$centers), type="b", ylim=c(0,1),
xlab="Serial Position", ylab="Proportion Correct")
#dev.off()
# kk <- {}
#
# for (nClust in 2:7){
#   kmres <- kmeans(spcdat, centers=nClust, nstart=10)
#   #kk <- c(kk, kmres$betweenss/kmres$totss)
#   kk <- c(kk,kmres$tot.withinss)
# }
#plot(2:7, kk, type="b", xlab="N Clusters", ylab="")
#plot some betas
curve(dbeta(x, 2, 4),ylim=c(0,6),ylab="Probability Density",las=1)
curve(dbeta(x, 8, 16),add=TRUE,lty="dashed")
legend("topright",c("Johnnie","Jane"), inset=.05,lty=c("solid","dashed"))
#the remaining lines are not listed in the book but perform some of the computations mentioned there
pbeta(.53,8,16)-pbeta(.13,8,16)
pbeta(.53,2,4)-pbeta(.13,2,4)
x11(7,7)
alpha <- beta <- 12
curve(dbeta(x, alpha, beta),ylim=c(0,40),ylab="Probability Density",las=1,lwd=3)
t<-c(12,100,1000)
i<-0
for (h in c(14,113,1130)){
i<-i+1
curve(dbeta(x, alpha+h, beta+t[i]),add=TRUE,lty=log10(t)+1)
print(c((alpha+h)/(alpha+h+beta+t[i]),h/(h+t[i])))
}
legend("topright",c("{14, 26}","{113, 213}", "{1130, 2130}"),
inset=.05,lty=c(2:4))
abline(v=0.5,col="red")
pbeta(.5,1130,1000)
pbeta(.5305164,1130,1000)
#plot some betas
curve(dbeta(x, 2, 4),ylim=c(0,6),ylab="Probability Density",las=1)
curve(dbeta(x, 8, 16),add=TRUE,lty="dashed")
legend("topright",c("Johnnie","Jane"), inset=.05,lty=c("solid","dashed"))
