#dat <- read.csv(file="rt_data.csv") # uncomment this if you read in data
nsubj <- 30
nobs <- 20
q_p <- c(.1,.3,.5,.7,.9)
shift <- rnorm(nsubj,250,50)
scale <- rnorm(nsubj,200,50)
shape <- rnorm(nsubj,2,0.25)
params <- rbind(shift,scale,shape)
print(rowMeans(params))
# rows are participants, columns are observations
dat <- apply(params, 2, function(x) rweibull(nobs,shape=x[3],scale=x[2])+x[1]) #* \label{line:MultipleParticipants:gendata}  *\#
# calculate sample quantiles for each particpant
kk <- apply(dat, 2, function(x) quantile(x, probs=q_p))
## FITTING VIA QUANTILE AVERAGING
# average the quantiles
vinq <- rowMeans(kk)
# fit the shifted Weibull to averaged quantiles
weib_qdev <- function(x,q_emp, q_p){
if (any(x<=0)){
return(10000000)
}
q_pred <- qweibull(q_p,shape=x[3],scale=x[2])+x[1] #* \label{line:MultipleParticipants:qweibull}  *\#
dev <- sqrt(mean((q_pred-q_emp)^2))
}
res <- optim(c(225,225,1),  #* \label{line:MultipleParticipants:qfit}  *\#
function(x) weib_qdev(x, vinq, q_p))
print(res)
## FITTING INDIVIDUAL PARTICIPANTS
weib_deviance <- function(x,rts){ #* \label{line:MultipleParticipants:weib_deviance}  *\#
if (any(x<=0) || any(rts<x[1])){
return(10000000)
}
likel <- dweibull(rts-x[1],shape=x[3],scale=x[2])
dev <- sum(-2*log(likel))
}
res <- apply(dat,2,function(a) optim(c(100,225,1),#* \label{line:MultipleParticipants:run_deviance}  *\#
function(x) weib_deviance(x, a)))
# Extract parameter estimates and put in to a matrix
parest <- matrix(
unlist(lapply(res, function(x) x$par)),
ncol=3, byrow=T)
print(colMeans(parest)) # mean parameter estimates
print(apply(parest,2,sd)) # SD of estimates
# note correlations between parameter estimates
myEM
library("ggplot2")
# generate some data
set.seed(1540614451)
N <- 1000
pShort <- 0.3
genpars <- list(c(100,10),
c(150,20))
# we assume equal sampling probability for the three distributions
whichD <- sample(c(1,2),N, replace=TRUE, prob=c(pShort, 1-pShort))
dat <- sapply(whichD, function(x)
rnorm(1,genpars[[x]][1],genpars[[x]][2]))
# function needed in EM
weighted.sd <- function(x,w,mu=mean(x)){
wvar <- sum(w*(x-mu)^2)/
sum(w)
return(sqrt(wvar))
}
# guess parameters
mu1 <- mean(dat,1)*0.8
mu2 <- mean(dat, 1)*1.2
sd1 <- sd(dat)
sd2 <- sd(dat)
ppi <- 0.5
oldppi <- 0
while (abs(ppi-oldppi)>.00001){ #* \label{line:MultipleParticipants:gmmloop}  *\#
oldppi <- ppi
# E step
resp <- ppi*dnorm(dat,mu2,sd2)/
((1-ppi)*dnorm(dat,mu1,sd1) + ppi*dnorm(dat,mu2,sd2))
# M step
mu1 <- weighted.mean(dat,1-resp)
mu2 <- weighted.mean(dat,resp)
sd1 <- weighted.sd(dat,1-resp,mu1)
sd2 <- weighted.sd(dat,resp,mu2)
ppi <- mean(resp)
print(ppi)
}
df <- data.frame(rt=dat)
#pdf(file="GMMexample.pdf", width=5, height=4)
ggplot(df, aes(x = rt)) +
geom_histogram(aes(y = ..density..),colour = "black", fill = "white",
binwidth = 3) +
stat_function(fun = function(k) (1-ppi)*dnorm(k,mu1,sd1)) +
stat_function(fun = function(k) ppi*dnorm(k,mu2,sd2)) +
xlab("RT (ms)") + ylab("Density")
#dev.off()
# mixtools for comparison
library(mixtools) # you'll need to install this library
myEM <- normalmixEM( dat, mu = c(1,4),
sigma=c(sd(dat),sd(dat)))
# Read in the data
# Rows are participants, columns are serial positions
# spcdat <- read.table("freeAccuracy.txt")
# Or generate some example data
nPrim <- 25
nRec <- 50
nBoth <- 25
ll <- 12
serpos <- 1:ll
nTrials <- 10
primDat <- matrix(rep(0,ll*nPrim),nPrim,ll)
for (j in 1:nPrim){
asym <- 0.3
expp <- 1
tdat <- (1-asym)*exp(-expp*(serpos-1)) + asym
primDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}
recDat <- matrix(rep(0,ll*nRec),nRec,ll)
for (j in 1:nRec){
asym <- 0.3
expp <- 1
tdat <- (1-asym)*exp(-expp*rev(serpos-1)) + asym
recDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}
bothDat <- matrix(rep(0,ll*nBoth),nBoth,ll)
for (j in 1:nBoth){
asym <- 0.5
expp <- 1
pc <- 0.5 * exp(-expp*rev(serpos-1)) + 0.5 * exp(-expp*(serpos-1))
tdat <- (1-asym)*pc + asym
bothDat[j,] <- rbinom(n=ll,size=nTrials,prob=tdat)/nTrials
}
spcdat <- rbind(primDat,recDat,bothDat)
#------------------------------------------
#pdf(file="gap_plot.pdf", width=4, height=4)
par(mfrow=c(1,1))
library(cluster) #* \label{line:MultipleParticipants:libclust}  *\#
gskmn <- clusGap(spcdat, FUN = kmeans, nstart = 20, K.max = 8, B=500)
plot(gskmn, ylim=c(0.15, 0.5))
#dev.off()
#-------------------------------------------
#pdf(file="kmeansSPC.pdf", width=8, height=4)
par(mfrow=c(1,2))
plot(colMeans(spcdat), ylim=c(0,1), type="b",
xlab="Serial Position", ylab="Proportion Correct", main=NULL)
kmres <- kmeans(spcdat, centers=3, nstart=10) #* \label{line:MultipleParticipants:runkmeans}  *\#
matplot(t(kmres$centers), type="b", ylim=c(0,1),
xlab="Serial Position", ylab="Proportion Correct")
#dev.off()
# kk <- {}
#
# for (nClust in 2:7){
#   kmres <- kmeans(spcdat, centers=nClust, nstart=10)
#   #kk <- c(kk, kmres$betweenss/kmres$totss)
#   kk <- c(kk,kmres$tot.withinss)
# }
#plot(2:7, kk, type="b", xlab="N Clusters", ylab="")
#plot some betas
curve(dbeta(x, 2, 4),ylim=c(0,6),ylab="Probability Density",las=1)
curve(dbeta(x, 8, 16),add=TRUE,lty="dashed")
legend("topright",c("Johnnie","Jane"), inset=.05,lty=c("solid","dashed"))
#the remaining lines are not listed in the book but perform some of the computations mentioned there
pbeta(.53,8,16)-pbeta(.13,8,16)
pbeta(.53,2,4)-pbeta(.13,2,4)
x11(7,7)
alpha <- beta <- 12
curve(dbeta(x, alpha, beta),ylim=c(0,40),ylab="Probability Density",las=1,lwd=3)
t<-c(12,100,1000)
i<-0
for (h in c(14,113,1130)){
i<-i+1
curve(dbeta(x, alpha+h, beta+t[i]),add=TRUE,lty=log10(t)+1)
print(c((alpha+h)/(alpha+h+beta+t[i]),h/(h+t[i])))
}
legend("topright",c("{14, 26}","{113, 213}", "{1130, 2130}"),
inset=.05,lty=c(2:4))
abline(v=0.5,col="red")
pbeta(.5,1130,1000)
pbeta(.5305164,1130,1000)
#perform SDT via ABC
#simulate sdt given parameters and number of trials
simsdt<- function(d,b,ntrials) {
old <- rnorm(ntrials/2,d)
hits <-sum(old>(d/2+b))/(ntrials/2)*100
new <- rnorm(ntrials/2,0)
fas <- sum(new>(d/2+b))/(ntrials/2)*100
return(X<-c(hits,fas))
}
y   <- c(60,11)  #define target data
dmu <- 1         #define hyperparameters
bmu <- 0
dsigma <- bsigma <- 1
ntrials <- 100
epsilon <- 1
posterior <- matrix(0,1000,2)
for (s in c(1:1000)) {  #commence ABC
while(TRUE) {
dprop <- rnorm(1,dmu,dsigma)
bprop <- rnorm(1,bmu,bsigma)
X<-simsdt(dprop,bprop,ntrials) #simulate proposal
if (sqrt(sum((y-X)^2)) <= epsilon) {break}
}
posterior[s,]<-c(dprop,bprop) #keep good simulation
print(s)                      #show sign of life
}
apply(posterior,2,mean)
apply(posterior,2,FUN=function(x) quantile(x,c(.025,.975)))
apply(posterior,2,hist)
print(c(s,sqrt(sum((y-X)^2)),X,posterior[s,]))
#random walk model
nreps <- 10000
nsamples <- 2000
drift <- 0.03  # 0 = noninformative stimulus; >0 = informative
sdrw <- 0.3
criterion <- 3
latencies <- rep(0,nreps)
responses <- rep(0,nreps)
evidence <- matrix(0, nreps, nsamples+1)
for (i in c(1:nreps)) {
evidence[i,] <- cumsum(c(0,rnorm(nsamples,drift,sdrw)))
p <-  which(abs(evidence[i,])>criterion)[1]
responses[i] <- sign(evidence[i,p])
latencies[i]  <- p
}
#plot up to 5 random walk paths
tbpn <- min(nreps,5)
plot(1:max(latencies[1:tbpn])+10,type="n",las=1,
ylim=c(-criterion-.5,criterion+.5),
ylab="Evidence",xlab="Decision time")
for (i in c(1:tbpn)) {
lines(evidence[i,1:(latencies[i]-1)])
}
abline(h=c(criterion,-criterion),lty="dashed")
#plot histograms of latencies
par(mfrow=c(2,1))
toprt <- latencies[responses>0]
topprop <- length(toprt)/nreps
hist(toprt,col="gray",
xlab="Decision time", xlim=c(0,max(latencies)),
main=paste("Top responses (",as.numeric(topprop),
") m=",as.character(signif(mean(toprt),4)),
sep=""),las=1)
botrt <- latencies[responses<0]
botprop <- length(botrt)/nreps
hist(botrt,col="gray",
xlab="Decision time",xlim=c(0,max(latencies)),
main=paste("Bottom responses (",as.numeric(botprop),
") m=",as.character(signif(mean(botrt),4)),
sep=""),las=1)
rm(list=ls())
#discrepancy for power forgetting function
powdiscrep <- function (parms,rec,ri) {
if (any(parms<0)||any(parms>1)) return(1e6)
pow_pred <- parms["a"] *(parms["b"]*ri + 1)^(-parms["c"])
return(sqrt( sum((pow_pred-rec)^2)/length(ri) ))
}
#Carpenter et al. (2008) Experiment 1
rec <- c(.93,.88,.86,.66,.47,.34)
ri  <- c(.0035, 1, 2, 7, 14, 42)
#initialize starting values
sparms <-c(1,.05,.7)
names(sparms) <- c("a","b","c")
#obtain best-fitting estimates
pout <- optim(sparms,powdiscrep,rec=rec,ri=ri)
pow_pred <- pout$par["a"] *(pout$par["b"]*c(0:max(ri)) + 1)^(-pout$par["c"])
#plot data and best-fitting predictions
x11()
par(cex.axis=1.2,cex.lab=1.4)
par(mar=(c(5, 5, 3, 2) + 0.1),las=1)
plot(ri,rec,
xlab = "Retention Interval (Days)",
ylab = "Proportion Items Retained",
ylim=c(0.3,1),xlim=c(0,43),xaxt="n",type="n")
lines(c(0:max(ri)),pow_pred,lwd=2)
points(ri,rec,pch=21, bg="dark grey",cex=2)
dev <- pow_pred[ri+1]
for (x in c(1:length(ri))) {
lines(c(ri[x],ri[x]),c(dev[x],rec[x]),lwd=1)
}
axis(1,at=c(0:43))
#perform bootstrapping analysis
ns  <- 55
nbs <- 1000
bsparms <- matrix(NA,nbs,length(sparms))
bspow_pred <- pout$par["a"] *(pout$par["b"]*ri + 1)^(-pout$par["c"])
for (i in c(1:nbs)) {
recsynth     <- vapply(bspow_pred, FUN=function(x) mean(rbinom(ns,1,x)), numeric(1))
bsparms[i,]  <- unlist(optim(pout$par,powdiscrep,rec=recsynth,ri=ri)$par)
}
#function to plot a histogram
histoplot<-function(x,l4x) {
hist(x,xlab=l4x,main="",xlim=c(0,1),cex.lab=1.5,cex.axis=1.5)
lq <- quantile(x,0.025)
abline(v=lq,lty="dashed",lwd=2)
uq <- quantile(x,0.975)
abline(v=uq,lty="dashed",lwd=2)
return(c(lq,uq))
}
x11(5,2)
par(mfcol=c(1,3),las=1)
for (i in c(1:dim(bsparms)[2])) {
print(histoplot(bsparms[,i],names(sparms)[i]))
}
#plot data and current predictions
getregpred <- function(parms,data) {
getregpred <- parms["b0"] + parms["b1"]*data[ ,2]
#wait with drawing a graph until key is pressed
par(ask=TRUE)
plot   (data[ ,2], type="n", las=1, ylim=c(-2,2), xlim=c(-2,2), xlab="X", ylab="Y")
par(ask=FALSE)
points (data[ ,2], data[ ,1], pch=21, bg="gray")
lines  (data[ ,2], getregpred, lty="solid")
return(getregpred)
}
#obtain current predictions and compute discrepancy
rmsd <-function(parms, data1) {
preds<-getregpred(parms, data1)
rmsd<-sqrt(sum((preds-data1[ ,1])^2)/length(preds))
}
#define parameters to generate data
nDataPts  <- 20
rho       <- .8
intercept <- .0
#generate synthetic data
data<-matrix(0,nDataPts,2)
data[ ,2] <- rnorm(nDataPts)
data[ ,1] <- rnorm(nDataPts)*sqrt(1.0-rho^2) + data[ ,2]*rho + intercept
#do conventional regression analysis
lm(data[,1] ~ data[,2])
#assign starting values
startParms <- c(-1., .2)
names(startParms) <- c("b1", "b0")
#obtain parameter estimates
xout <- optim(startParms, rmsd, data1=data)
