choice_p[which.max(mins)] <- 1-alpha
return(choice_p)
}
# then compare probability of minimums
p_of_mins <- c(prospect[[1]]$p[which.min(prospect[[1]]$x)],
prospect[[2]]$p[which.min(prospect[[2]]$x)])
if (abs(p_of_mins[1]-p_of_mins[2])>0.1){
choice_p <- rep(alpha,2)
choice_p[which.min(p_of_mins)] <- 1-alpha
return(choice_p)
}
# then compare maximums
maxs <- c(max(prospect[[1]]$x),max(prospect[[2]]$x))
choice_p <- rep(alpha,2)
choice_p[which.max(maxs)] <- 1-alpha
return(choice_p)
}
}
n <- 20
n_transfer <- 10
nReps <- 500
maxOrder <- 10
realOrder <- 3
m <- 1:n
bias <- rep(0, maxOrder)
variab <- rep(0,maxOrder)
tcoef <- c(1,-1,1)
kk <- poly(m,length(tcoef))
realf <- kk%*%tcoef
allPreds <- {}
for (modOrder in 1:maxOrder){
allPreds[[modOrder]] <- matrix(rep(0,n*nReps),ncol=n)
}
for (rep in 1:nReps){
#tcoef <- c(-1,-1,1)
dat <- realf + rnorm(n, sd=0.5)
for (modOrder in 1:maxOrder){
lm_t <- lm(dat ~ poly(m,modOrder))
allPreds[[modOrder]][rep,] <- predict(lm_t)
}
}
#pdf(file="biasVarianceIllus.pdf",width=7,height=7)
par(mfrow=c(2,2))
for (ii in c(1,3,7,10)){
plot(m,realf,ylim=c(-1.5,1),
type="b",lwd=2,las=1,
xlab="x", ylab="y")
lines(colMeans(allPreds[[ii]]),
col=rgb(0,0,0,0.5),lwd=3)
kk <- apply(allPreds[[ii]],2,function(x) quantile(x,c(0.1,0.9)))
lines(smooth(kk[1,]), lwd=2,lty=2,col=rgb(0,0,0,0.5))
lines(smooth(kk[2,]), lwd=2,lty=2,col=rgb(0,0,0,0.5))
legend(10,-0.5,legend=c("true","fitted"), lty=1,
col=c("black","grey"),lwd=2)
title(paste("Order = ",ii))
}
#dev.off()
bias <- rep(0,maxOrder)
variab <- rep(0,maxOrder)
for (ii in 1:maxOrder){
mm <- colMeans(allPreds[[ii]])
bias[ii] <- mean((mm-realf)^2)
kk <- apply(allPreds[[ii]],1,function(x) (x-mm)^2)
variab[ii] <- mean(rowMeans(kk))
}
#pdf(file="biasVarTradeOff.pdf",width=5,height=5)
par(mfrow=c(1,1))
plot(bias+variab, type="l", lwd=2, lty=2,
ylim=c(0,0.15),
xlab="Complexity", ylab="Error")
lines(bias,lwd=2)
lines(variab,lwd=2,col=rgb(0,0,0,0.5))
legend(6,0.07,
legend=c("Total Error",expression("Bias" ^ "2"),"Variance"),
lty=c(2,1,1),lwd=1,col=c("black","black","grey"))
#dev.off()
# ---------Cumulative prospect theory-------#
# probability weighting function
probw <- function(p,c){
return(p^c/
((p^c + (1-p)^c)^(1/c))
)
}
# CPT calling fn: calculates subjective value of a prospect
# Assumes x are unique (i.e., no ties)
cumulPTv <- function(x, p, alpha=1,beta=1,lambda=1,gamma=1,delta=1){
if ((length(x)<1) || (length(p)<1) || (length(x)!=length(p))){
print(x)
print(p)
stop("x and p must be of the same length (>1)")
}
oo <- order(x)
x <- x[oo]
p <- p[oo]
# ---- deal with x>= 0
ii <- which(x>=0)
if (length(ii)>0){
tx <- x[ii]
tp <- p[ii]
vp <- tx^alpha
pp <- {}
pp[length(tp)] <- probw(tp[length(tp)],gamma)
if (length(ii)>1){
for (j in 1:(length(ii)-1)){
pp[j] <- probw(sum(tp[j:length(tp)]),gamma)-
probw(sum(tp[(j+1):length(tp)]),gamma)
}
}
sv_pos <- sum(vp*pp)
} else {
sv_pos <- 0
vp <- {}
pp <- {}
}
# ---- deal with x< 0
ii <- which(x<0)
if (length(ii)>0){
tx <- x[ii]
tp <- p[ii]
vn <- -lambda*(-tx)^beta
pn <- {}
pn[1] <- probw(tp[1],delta)
if (length(ii)>1){
for (i in 2:length(ii)){
pn[i] <- probw(sum(tp[1:i]),delta)-
probw(sum(tp[1:(i-1)]),delta)
}
}
sv_neg <- sum(vn*pn)
} else {
sv_neg <- 0
vn <- {}
pn <- {}
}
# calculate subjective value of prospect
sv <- sv_pos + sv_neg
return(list(sv=sv,v=c(vn,vp),pip=c(pn,pp)))
}
CPTchoice <- function(prospect,alpha=1,beta=1,lambda=1,gamma=1,delta=1,phi=1){
# prospect is a list of alternative gambles, each gamble holding a vector of values x and a vector of probabilities p
SVs <- lapply(prospect,
function(tx) cumulPTv(tx$x, tx$p, alpha,beta,lambda,gamma,delta)$sv)
scalSVs <- exp(phi*unlist(SVs))
return(scalSVs/sum(scalSVs))
}
source("cumulPT.R")
library(dfoptim)
# function to calculate lnL for CPT
fitCPT <- function(theta, prospects,choices){
lnL <- rep(0,length(prospects))
for (i in 1:length(prospects)){
cprobs <- CPTchoice(prospects[[i]],
theta[1],theta[2],theta[3],theta[4],theta[5],theta[6])
if (!is.vector(choices)){
lnL[i] <- sum(log(cprobs[choices[i,]+1]))
} else {
lnL[i] <- log(cprobs[choices[i]+1])
}
}
if (any(is.infinite(lnL) | is.na(lnL))){
return(10000)
} else {
return(-2*sum(lnL))
}
}
dat <- read.csv("Rieskamp2008data.csv",
header=T)
prospects <- {}
for (i in 1:length(dat$choicepair)){
p1 <- list(x=c(dat$A1_payoff[i],
dat$A2_payoff[i]),
p=c(dat$A1_prob[i],
dat$A2_prob[i]))
p2 <- list(x=c(dat$B1_payoff[i],
dat$B2_payoff[i]),
p=c(dat$B1_prob[i],
dat$B2_prob[i]))
prospects[[i]] <- list(p1=p1,p2=p2)
}
choices <- subset(dat, select=X1:X30)
# fit individuals with lambda free
startPoints <- as.matrix(expand.grid(alpha=c(0.7, 0.9),
lambda=c(0.7, 1.4),
gamma=c(0.5,0.8),
delta=c(0.5,0.8),
phi=c(0.05,2)))
fits <- {}
for (subj in 1:30){
tchoice <- choices[,subj]
print(paste('Fitting subject ',subj))
bfit <- list(value=10000)
for (sp in 1:dim(startPoints)[1]){
tfit <- nmkb(par=startPoints[sp,],
fn = function(theta) fitCPT(c(theta[1],theta[1],theta[2:5]),
prospects=prospects, choices=tchoice),
lower=c(0,0,0,0,0),
upper=c(1,10,1,1,10),
control=list(trace=0))
if (tfit$value < bfit$value){
bfit <- tfit
}
print(paste(sp,tfit$value,bfit$value))
}
fits[[subj]] <- bfit
}
source("priorityHeuristic.R")
library(dfoptim)
# function to calculate lnL for priority heuristic with naive
# noise (Rieskamp, 2008)
fitPriority <- function(alpha, prospects,choices){
lnL <- rep(0,length(prospects))
for (i in 1:length(prospects)){
cprobs <- priorityHeuristic(prospects[[i]],
alpha)
if (!is.vector(choices)){
lnL[i] <- sum(log(cprobs[choices[i,]+1]))
} else {
lnL[i] <- log(cprobs[choices[i]+1])
}
}
if (any(is.infinite(lnL) | is.na(lnL))){
return(10000)
} else {
return(-2*sum(lnL))
}
}
dat <- read.csv("Rieskamp2008data.csv",
header=T)
prospects <- {}
for (i in 1:length(dat$choicepair)){
p1 <- list(x=c(dat$A1_payoff[i],
dat$A2_payoff[i]),
p=c(dat$A1_prob[i],
dat$A2_prob[i]))
p2 <- list(x=c(dat$B1_payoff[i],
dat$B2_payoff[i]),
p=c(dat$B1_prob[i],
dat$B2_prob[i]))
prospects[[i]] <- list(p1=p1,p2=p2)
}
choices <- subset(dat, select=X1:X30)
fits <- {}
for (subj in 1:30){
tchoice <- choices[,subj]
print(paste('Fitting subject ',subj))
bfit <- list(value=10000)
# fits[[subj]] <- optim(par=c(0.9,0.9,1,.77,.77,0.25),fn = fitCPT,
#                        prospects=prospects, choices=tchoice,
#                        method="L-BFGS-B",
#                        lower=c(0,0,0,0.0,0.0,0),
#                        upper=c(1,1,10,1,1,10),
#                        control=list(trace=1))
# fits[[subj]] <- nmkb(par=c(0.9,0.9,1.1,.7,.77,0.1),fn = fitCPT,
#                       prospects=prospects, choices=tchoice,
#                       lower=c(0,0,1,0.4,0.4,0),
#                       upper=c(1,1,10,1,1,10),
#                       control=list(trace=1))
# alpha=beta
fits[[subj]] <- optim(par=0.25,
fn = function(alpha) fitPriority(alpha,
prospects=prospects, choices=tchoice),
method="Brent",
lower=0,
upper=0.5,
control=list(trace=0))
}
alpha_est <- do.call(rbind,lapply(fits,function(tx) tx$par))
lnLs <- do.call(rbind,lapply(fits,function(tx) tx$value))
n <- 1000
m <- seq(1,10,length.out = n)
#pdf(file="polycomplex.pdf", width=7, height=4)
par(mfrow=c(1,2))
for (modOrder in c(2,10)){
plot(1:10,1:10,xlim=c(0.5,10.5), ylim=c(0,5), type="n",
xlab="Physical intensity", ylab="Sensation", las=1)
kk <- poly(x = m,degree = modOrder)
for (rep in 1:10){
tcoef <- rnorm(modOrder, sd=5)
#tcoef <- rexp(modOrder, rate=0.2)
y <- t(kk%*%tcoef)
y <- y - min(y)
lines(x=m, y, type="l", col=rgb(0,0,0,0.5))
}
}
#dev.off()
n <- 10
stimrange <- c(1,10)
m <- seq(stimrange[1],stimrange[2],length.out = n)
s <- log(m) + rnorm(n,sd=0.5)
s[s < 0] <- 0
lm_3 <- lm(s ~ poly(m,3))
lm_n <- lm(s ~ poly(m,n-1))
#pdf(file="Nihm76.pdf", width=6, height=6)
par(mfrow=c(2,2))
plot(m,s, pch=3, las=1,
xlab='Physical Intensity',
ylab='Perceived Intensity')
points(predict(lm_3))
plot(m,s, pch=3, las=1,
xlab='Physical Intensity',
ylab='Perceived Intensity')
points(predict(lm_n))
m_fill <- seq(stimrange[1],stimrange[2],length.out = 1000)
plot(m,s, pch=3, las=1,
xlab='Physical Intensity',
ylab='Perceived Intensity')
lines(m_fill,
predict(lm_3,data.frame(m=m_fill)),
type="l", lty=1, las=1)
plot(m,s, pch=3, las=1,
xlab='Physical Intensity',
ylab='Perceived Intensity')
lines(m_fill,
predict(lm_n,data.frame(m=m_fill)),
type="l", lty=1, las=1)
#dev.off()
n <- 20
n_transfer <- 2
nReps <- 1000
maxOrder <- 6
trainErr <- rep(0, maxOrder)
testErr <- rep(0, maxOrder)
m <- 1:n
tcoef <- c(1,-1,1)
kk <- poly(m,length(tcoef))
realf <- t(kk%*%tcoef)
plot(m, realf)
for (rep in 1:nReps){
#tcoef <- c(-1,-1,1)
dat <- realf + rnorm(n, sd=0.5)
m_t <- sample.int(n, size=n_transfer)
m <- setdiff(1:n,m_t)
for (modOrder in 1:maxOrder){
tTrain <- 0
tTest <- 0
lm_t <- lm(dat[m] ~ poly(m,modOrder))
tTrain <- tTrain + mean(lm_t$residuals^2)
tTest <- tTest + mean((predict(lm_t, data.frame(m=m_t))-dat[m_t])^2)
trainErr[modOrder] <- trainErr[modOrder] + tTrain
testErr[modOrder] <- testErr[modOrder] + tTest
}
}
kk <- cbind(trainErr,testErr)/nReps
#pdf(file="outOfSet.pdf",width=4,height=5)
matplot(kk, type="l",lty=1,
ylim=c(0,0.7),
lwd=2,col=c("black","grey"),
xlab="Complexity",ylab="Prediction Error")
legend(1,0.7,c("Train","Test"),lwd=2,lty=1,col=c("black","grey"))
#dev.off()
priorityHeuristic <- function(prospect,alpha=0.1){
# priority heuristic for two gambles
allx <- c(prospect[[1]]$x, prospect[[2]]$x)
if (all(allx<0)){ # all negative
# first compare minimums
max_x <- min(allx)
mins <- c(max(prospect[[1]]$x),max(prospect[[2]]$x))
if ((abs(mins[1]-mins[2])/abs(max_x))>0.1){
choice_p <- rep(alpha,2)
choice_p[which.max(mins)] <- 1-alpha
return(choice_p)
}
# then compare probability of minimums
p_of_mins <- c(prospect[[1]]$p[which.max(prospect[[1]]$x)],
prospect[[2]]$p[which.max(prospect[[2]]$x)])
if (abs(p_of_mins[1]-p_of_mins[2])>0.1){
choice_p <- rep(alpha,2)
choice_p[which.min(p_of_mins)] <- 1-alpha
return(choice_p)
}
# then compare maximums
maxs <- c(max(prospect[[1]]$x),max(prospect[[2]]$x))
choice_p <- rep(alpha,2)
choice_p[which.max(maxs)] <- 1-alpha
return(choice_p)
} else { # mixed or positive
# first compare minimums
max_x <- max(allx)
mins <- c(min(prospect[[1]]$x),min(prospect[[2]]$x))
if ((abs(mins[1]-mins[2])/max_x)>0.1){
choice_p <- rep(alpha,2)
choice_p[which.max(mins)] <- 1-alpha
return(choice_p)
}
# then compare probability of minimums
p_of_mins <- c(prospect[[1]]$p[which.min(prospect[[1]]$x)],
prospect[[2]]$p[which.min(prospect[[2]]$x)])
if (abs(p_of_mins[1]-p_of_mins[2])>0.1){
choice_p <- rep(alpha,2)
choice_p[which.min(p_of_mins)] <- 1-alpha
return(choice_p)
}
# then compare maximums
maxs <- c(max(prospect[[1]]$x),max(prospect[[2]]$x))
choice_p <- rep(alpha,2)
choice_p[which.max(maxs)] <- 1-alpha
return(choice_p)
}
}
#rm(list=ls())
#setwd("C:/Users/Lewan/Documents/Papers Written/Modeling Book/_Big Book/bigbook/BayesianJAGS")
#next line must be L5
library(rjags)
#provide data from experiment
h <- 60
f <- 11
sigtrials <- noistrials <- 100
#define JAGS model
onehtj <- jags.model("1HT.j",
data = list("h"=h, "f"=f,
"sigtrials"=sigtrials,
"noistrials"=noistrials),
n.chains=4)
source("cumulPT.R")
library(dfoptim)
# function to calculate lnL for CPT
fitCPT <- function(theta, prospects,choices){
lnL <- rep(0,length(prospects))
for (i in 1:length(prospects)){
cprobs <- CPTchoice(prospects[[i]],
theta[1],theta[2],theta[3],theta[4],theta[5],theta[6])
if (!is.vector(choices)){
lnL[i] <- sum(log(cprobs[choices[i,]+1]))
} else {
lnL[i] <- log(cprobs[choices[i]+1])
}
}
if (any(is.infinite(lnL) | is.na(lnL))){
return(10000)
} else {
return(-2*sum(lnL))
}
}
dat <- read.csv("Rieskamp2008data.csv",
header=T)
prospects <- {}
for (i in 1:length(dat$choicepair)){
p1 <- list(x=c(dat$A1_payoff[i],
dat$A2_payoff[i]),
p=c(dat$A1_prob[i],
dat$A2_prob[i]))
p2 <- list(x=c(dat$B1_payoff[i],
dat$B2_payoff[i]),
p=c(dat$B1_prob[i],
dat$B2_prob[i]))
prospects[[i]] <- list(p1=p1,p2=p2)
}
choices <- subset(dat, select=X1:X30)
# fit individuals with lambda free
startPoints <- as.matrix(expand.grid(alpha=c(0.7, 0.9),
lambda=c(0.7, 1.4),
gamma=c(0.5,0.8),
delta=c(0.5,0.8),
phi=c(0.05,2)))
fits <- {}
for (subj in 1:30){
tchoice <- choices[,subj]
print(paste('Fitting subject ',subj))
bfit <- list(value=10000)
for (sp in 1:dim(startPoints)[1]){
tfit <- nmkb(par=startPoints[sp,],
fn = function(theta) fitCPT(c(theta[1],theta[1],theta[2:5]),
prospects=prospects, choices=tchoice),
lower=c(0,0,0,0,0),
upper=c(1,10,1,1,10),
control=list(trace=0))
if (tfit$value < bfit$value){
bfit <- tfit
}
print(paste(sp,tfit$value,bfit$value))
}
fits[[subj]] <- bfit
}
# fit individuals with lambda fixed at 1
startPoints <- as.matrix(expand.grid(alpha=c(0.7, 0.9),
gamma=c(0.5,0.8),
delta=c(0.5,0.8),
phi=c(0.05,2)))
fits_con <- {}
for (subj in 1:30){
tchoice <- choices[,subj]
print(paste('Fitting subject ',subj))
bfit <- list(value=10000)
# alpha=beta, labda=1
for (sp in 1:dim(startPoints)[1]){
tfit <- nmkb(par=startPoints[sp,],
fn = function(theta) fitCPT(c(theta[1],theta[1],1,theta[2:4]),
prospects=prospects, choices=tchoice),
lower=c(0,0,0,0),
upper=c(1,1,1,10),
control=list(trace=0))
if (tfit$value < bfit$value){
bfit <- tfit
}
print(paste(sp,tfit$value,bfit$value))
}
fits_con[[subj]] <- bfit
}
save('fits_con', file="myFits_con.Rdata")
library(xtable)
parms_full <- do.call(rbind,lapply(fits,function(tx) tx$par))
parms_con <- do.call(rbind,lapply(fits_con,function(tx) tx$par))
# overall
lnLs_full <- do.call(rbind,lapply(fits,function(tx) tx$value))
lnLs_con <- do.call(rbind,lapply(fits_con,function(tx) tx$value))
sum(lnLs_con)-sum(lnLs_full)
print(apply(parms_full,2,mean),digits=2)
print(apply(parms_full,2,sd),digits=2)
print(apply(parms_con,2,mean),digits=2)
print(apply(parms_con,2,sd),digits=2)
# individual
parms_full[(lnLs_con-lnLs_full) > qchisq(.95,1),2]
